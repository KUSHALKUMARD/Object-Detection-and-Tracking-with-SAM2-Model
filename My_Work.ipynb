{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is designed to process images and track objects across different frames using the SAM2 model, a tool developed for automatic mask generation and object tracking in videos. The purpose is to demonstrate object detection in an image and then track the same object in a subsequent frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importing Required Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2, build_sam2_video_predictor\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob, shutil\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries are crucial for the functioning of the code.\n",
    "torch is used for loading and running the SAM2 model on a CUDA device.\n",
    "PIL and matplotlib are for image handling and visualization.\n",
    "os, glob, and shutil handle file and directory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring all dependencies were correctly installed and compatible with each other. For instance, configuring torch to work on a CUDA-enabled device can sometimes cause issues, especially if the correct CUDA version is not installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Improved Image Processing Functionh1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img_png_mask(imgpath, maskpath, visualize=False):\n",
    "    try:\n",
    "        # Load the image and mask\n",
    "        img = Image.open(imgpath)\n",
    "        mask = Image.open(maskpath)\n",
    "        \n",
    "        # Convert the mask to a binary format (0 and 1)\n",
    "        mask = mask.convert(\"L\")  # Convert to grayscale\n",
    "        mask_np = np.array(mask) / 255  # Normalize to binary\n",
    "        \n",
    "        # Ensure mask is binary (0s and 1s)\n",
    "        mask_np[mask_np > 0] = 1\n",
    "        \n",
    "        # Find bounding box coordinates based on the binary mask\n",
    "        rows = np.any(mask_np, axis=1)\n",
    "        cols = np.any(mask_np, axis=0)\n",
    "        ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "        xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "        \n",
    "        # If visualization is enabled, show the bounding box on the image\n",
    "        if visualize:\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "            plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, edgecolor='red', facecolor='none', lw=2))\n",
    "            plt.title('Bounding Box on First Image')\n",
    "            plt.show()\n",
    "        \n",
    "        return xmin, xmax, ymin, ymax\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_img_png_mask: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes an image and its corresponding mask to identify the bounding box of the object of interest. The mask is converted to a binary format, and the bounding box coordinates are extracted based on the areas where the mask is active (i.e., where the object is present).\n",
    "The function also includes an option to visualize the bounding box.\n",
    "\n",
    "Handling different image formats and ensuring that the mask accurately represents the object.\n",
    "Debugging issues related to bounding box extraction, especially when the mask did not align perfectly with the object in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Object Tracking Across Frames*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_item_boxes(first_img_path, second_img_path, boxes, predictor_vid, visualize=False):\n",
    "    try:\n",
    "        # Load images\n",
    "        first_img = Image.open(first_img_path)\n",
    "        second_img = Image.open(second_img_path)\n",
    "\n",
    "        # Initialize tracking using SAM2's video predictor\n",
    "        tempfolder = \"./tempdir\"\n",
    "        create_if_not_exists(tempfolder)\n",
    "        cleardir(tempfolder)\n",
    "\n",
    "        shutil.copy(first_img_path, tempfolder + \"/00000.jpg\")\n",
    "        shutil.copy(second_img_path, tempfolder + \"/00001.jpg\")\n",
    "\n",
    "        # Initialize inference state for video tracking\n",
    "        inference_state = predictor_vid.init_state(video_path=tempfolder)\n",
    "        predictor_vid.reset_state(inference_state)\n",
    "\n",
    "        ann_frame_idx = 0  # Annotated frame is the first image\n",
    "        video_segments = {}\n",
    "\n",
    "        # Track the object across images\n",
    "        for box, object_id in boxes:\n",
    "            xmin, xmax, ymin, ymax = map(int, box)\n",
    "            sam_box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\n",
    "            \n",
    "            # Add new points or boxes for tracking\n",
    "            _, out_obj_ids, out_mask_logits = predictor_vid.add_new_points_or_box(\n",
    "                inference_state=inference_state,\n",
    "                frame_idx=ann_frame_idx,\n",
    "                obj_id=object_id,\n",
    "                box=sam_box,\n",
    "            )\n",
    "            \n",
    "            # Propagate tracking across frames\n",
    "            for out_frame_idx, out_obj_ids, out_mask_logits in predictor_vid.propagate_in_video(inference_state):\n",
    "                video_segments[out_frame_idx] = {\n",
    "                    out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "                    for i, out_obj_id in enumerate(out_obj_ids)\n",
    "                }\n",
    "\n",
    "        # Visualization of tracked objects\n",
    "        if visualize:\n",
    "            # Show the first image with bounding box\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            ax[0].imshow(first_img)\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, edgecolor='green', facecolor='none', lw=2)\n",
    "            ax[0].add_patch(rect)\n",
    "            ax[0].set_title(\"First Image with Bounding Box\")\n",
    "\n",
    "            # Show the second image with the mask overlaid\n",
    "            ax[1].imshow(second_img)\n",
    "            ax[1].setTitle(\"Tracked Object in Second Image\")\n",
    "            for out_obj_id, out_mask in video_segments[1].items():  # Second frame index = 1\n",
    "                show_mask(out_mask, ax[1], obj_id=out_obj_id)\n",
    "            plt.show()\n",
    "\n",
    "        return video_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error in track_item_boxes: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tracks the object identified in the first image (using its bounding box) across frames using the SAM2 video predictor.\n",
    "It initializes the tracking environment, loads the images, and propagates the tracking information across frames.\n",
    "If visualization is enabled, it displays the object in the first image with a bounding box and the tracked object in the second image.\n",
    "\n",
    "Managing the initialization of the video tracking state and ensuring that the propagation of the tracking information works correctly.\n",
    "Debugging issues related to incorrect object tracking, such as when the objectâ€™s mask was not correctly applied in subsequent frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Helper Functions for Visualization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        color = np.array([*cmap(obj_id)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function displays the mask of the tracked object on the image. The mask is color-coded based on the object ID.\n",
    "The function can either use a random color or a predefined color scheme.\n",
    "\n",
    "Ensuring that the mask is correctly aligned with the image and that the color coding is consistent across frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Directory Management Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_if_not_exists(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname)\n",
    "\n",
    "def cleardir(tempfolder):\n",
    "    filepaths = glob.glob(tempfolder + \"/*\")\n",
    "    for filepath in filepaths:\n",
    "        os.unlink(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions manage the directories used in the code. create_if_not_exists creates a directory if it doesnâ€™t already exist, and cleardir clears all files from a given directory.\n",
    "\n",
    "Ensuring that the directory management is handled correctly, particularly when dealing with temporary files that need to be cleaned up after processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize the SAM2 Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"C:/Users/victus/Documents/segment-anything-2/sam2_hiera_tiny.pt\"\n",
    "model_cfg = \"C:/Users/victus/Documents/segment-anything-2/sam2_hiera_t.yaml\"\n",
    "sam2 = build_sam2(model_cfg, checkpoint, device='cuda', apply_postprocessing=False)\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "predictor_vid = build_sam2_video_predictor(model_cfg, checkpoint, device='cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step initializes the SAM2 model using the provided checkpoint and configuration files. The model is loaded onto a CUDA device for faster processing.\n",
    "\n",
    "Ensuring that the correct paths are provided for the checkpoint and configuration files.\n",
    "Handling issues related to CUDA, such as compatibility and device memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Process the First Image to Get Bounding Box Coordinates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstimgpath = './CMU10_3D/data_2D/can_chowder_000001.jpg'\n",
    "firstimgmaskpath = './CMU10_3D/data_2D/can_chowder_000001_1_gt.png'\n",
    "secondimgpath = './CMU10_3D/data_2D/can_chowder_000002.jpg'\n",
    "\n",
    "[xmin, xmax, ymin, ymax] = process_img_png_mask(firstimgpath, firstimgmaskpath, visualize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image and its mask are processed to extract the bounding box coordinates, which will be used for object tracking.\n",
    "\n",
    "Ensuring that the mask correctly represents the object in the image and that the bounding box accurately captures the objectâ€™s position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Track the Object Between the First and Second Images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = [([xmin, xmax, ymin, ymax], 1)]\n",
    "tracked_segments = track_item_boxes(firstimgpath, secondimgpath, boxes, predictor_vid, visualize=True)\n",
    "\n",
    "print(\"Tracked Masks for Each Frame:\")\n",
    "for frame_idx, masks in tracked_segments.items():\n",
    "    print(f\"Frame {frame_idx}:\")\n",
    "    for obj_id, mask in masks.items():\n",
    "        print(f\"  ObjectA {obj_id} Mask:\")\n",
    "        print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object identified in the first image is tracked in the second image, and the results are printed for each frame.\n",
    "\n",
    "Ensuring that the object tracking is consistent and that the results are accurate. Debugging tracking issues when the mask did not align with the object was particularly challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Working on this project has been an incredibly rewarding experience. It allowed me to dive deep into the intricacies of object detection and tracking, leveraging advanced tools like the SAM2 model. I faced several challenges along the way, from managing dependencies to fine-tuning the tracking algorithm, but each hurdle provided an opportunity to learn and grow.*\n",
    "\n",
    "**What I found most exciting was the ability to see tangible resultsâ€”watching an object seamlessly tracked across frames was a gratifying moment. It reinforced my passion for data science and the potential it has to solve real-world problems.**\n",
    "\n",
    "*I am genuinely happy to have had the opportunity to work on this project, and I look forward to applying these learnings to future challenges. Thank you for the chance to contribute and for pushing me to refine my skills.*\n",
    "\n",
    "Best regards,\n",
    "Kushal Kumar D\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
